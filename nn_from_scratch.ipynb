{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install python-mnist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9B_F6tkdzrY",
        "outputId": "b3604ac9-8bc3-4b61-9509-f6401464c767"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-mnist\n",
            "  Downloading python_mnist-0.7-py2.py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: python-mnist\n",
            "Successfully installed python-mnist-0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5iQY-SW3B7pF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mnist import MNIST"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTCC0V1tdTn1",
        "outputId": "3164c892-c469-48ec-bece-12db95161c03"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mB4wzybMbseq"
      },
      "outputs": [],
      "source": [
        "# Reading the dataset\n",
        "mndata = MNIST('/content/drive/MyDrive/mnist_samples')\n",
        "\n",
        "X_train, y_train = mndata.load_training()\n",
        "X_test, y_test = mndata.load_testing()\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5WbFgBE0CJmP"
      },
      "outputs": [],
      "source": [
        "class Layer_Dense():\n",
        "    def __init__(self, n_inputs, n_neurons):\n",
        "\n",
        "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "        self.inputs = inputs # we want to remember what our inputs are\n",
        "\n",
        "    def backward(self, dvalues):\n",
        "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "        # for easier understanding, we can calculate dbiases with\n",
        "        # np.dot(dvalues, np.array([1, 1, 1]).T) (second argument is derivative of z with respect to biases for all samples)\n",
        "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "        self.dinputs = np.dot(dvalues, self.weights.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VcCsDY4vCRR1"
      },
      "outputs": [],
      "source": [
        "class Activation_ReLU():\n",
        "    def forward(self, inputs):\n",
        "        self.output = np.maximum(0, inputs)\n",
        "        self.inputs = inputs\n",
        "\n",
        "    def backward(self, dvalues):\n",
        "        \"\"\"\n",
        "        the same but with multiplication and more clearly:\n",
        "\n",
        "        drelu_dinputs[inputs <= 0] = 0\n",
        "        drelu_dinputs[inputs > 0] = 1\n",
        "        dinputs = dvalues * drelu_dinputs\n",
        "        \"\"\"\n",
        "        self.dinputs = dvalues.copy()\n",
        "        self.dinputs[self.inputs <= 0] = 0\n",
        "\n",
        "class Activation_Softmax():\n",
        "    def forward(self, inputs):\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "        probabilities = exp_values / exp_values.sum(axis=1, keepdims=True)\n",
        "        self.output = probabilities\n",
        "\n",
        "        # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Create uninitialized array\n",
        "        self.dinputs = np.empty_like(dvalues)\n",
        "        # Enumerate outputs and gradients\n",
        "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
        "            # Flatten output array\n",
        "            single_output = single_output.reshape(-1, 1)\n",
        "            # Calculate Jacobian matrix of the output and\n",
        "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
        "\n",
        "            # Calculate sample-wise gradient\n",
        "            # and add it to the array of sample gradients\n",
        "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZTfgoZtRCYyi"
      },
      "outputs": [],
      "source": [
        "class Loss():\n",
        "    def calculate(self, output, y):\n",
        "        sample_losses = self.forward(output, y)\n",
        "        data_loss = np.mean(sample_losses)\n",
        "        return data_loss\n",
        "\n",
        "class Loss_CategoricalCrossentropy(Loss):\n",
        "    def forward(self, y_pred, y_true):\n",
        "        self.y_pred = y_pred\n",
        "        self.y_true = y_true\n",
        "\n",
        "        samples = len(y_pred)\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        if len(y_true.shape) == 1:\n",
        "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
        "        elif len(y_true.shape) == 2:\n",
        "            correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n",
        "\n",
        "        return -np.log(correct_confidences)\n",
        "\n",
        "    def backward(self, dvalues, y_true):\n",
        "        # dvalues actually is a matrix which contains predictions (samples x predictions)\n",
        "        samples = len(dvalues)\n",
        "        # Number of labels in every sample\n",
        "        # We'll use the first sample to count them\n",
        "        labels = dvalues[0]\n",
        "        # If labels are sparse, turn them into one-hot vector\n",
        "        if len(y_true.shape) == 1:\n",
        "            y_true = np.eye(labels)[y_true]\n",
        "\n",
        "        dinputs = - y_true / dvalues\n",
        "\n",
        "        # this normalization step ensures that the gradients are averaged across all samples\n",
        "        dinputs = dinputs / samples\n",
        "\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
        "    def __init__(self):\n",
        "        self.activation = Activation_Softmax()\n",
        "        self.loss = Loss_CategoricalCrossentropy()\n",
        "\n",
        "    def forward(self, inputs, y_true):\n",
        "        # Output layer's activation function\n",
        "        self.activation.forward(inputs)\n",
        "        # Set the output\n",
        "        self.output = self.activation.output\n",
        "        # Calculate and return loss value\n",
        "        return self.loss.calculate(self.output, y_true)\n",
        "\n",
        "        # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "        # derivative of softmax + loss is (y_hat - y_true)\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "\n",
        "        if len(y_true.shape) == 2:\n",
        "            y_true = np.argmax(y_true, axis=1)\n",
        "        # Copy so we can safely modify\n",
        "        self.dinputs = dvalues.copy()\n",
        "        # Calculate gradient\n",
        "        self.dinputs[range(samples), y_true] -= 1\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JMw0tox_CgYv"
      },
      "outputs": [],
      "source": [
        "class Optimizer_SGD():\n",
        "    def __init__(self, learning_rate=1.0, decay=.0, momentum=0.):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.momentum = momentum\n",
        "        self.iterations = 0\n",
        "\n",
        "    # function for updating learning rate\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            # every time we multiply our learning rate (for example 1) with smaller and smaller float (<=1)\n",
        "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    def update_params(self, layer):\n",
        "        # if we use momentum\n",
        "        if self.momentum:\n",
        "            # If layer does not contain momentum arrays, create them\n",
        "            if not hasattr(layer, 'weight_momentums'):\n",
        "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "\n",
        "            # while updating our wwights we take into count our prevous gradients\n",
        "            # momentum walue determines how much we want to keep out prevous gradients\n",
        "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
        "            layer.weight_momentums = weight_updates\n",
        "\n",
        "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
        "            layer.bias_momentums = bias_updates\n",
        "\n",
        "        else: # If we dont use momentum\n",
        "            weight_updates = -self.learning_rate * layer.dweights\n",
        "            bias_updates = -self.learning_rate * layer.dbiases\n",
        "\n",
        "        layer.weights += weight_updates\n",
        "        layer.biases += bias_updates\n",
        "\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lbtm-K19bse9"
      },
      "outputs": [],
      "source": [
        "class Optimizer_AdaGrad():\n",
        "    def __init__(self, learning_rate=1.0, decay=.0, epsilon=1e-7):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    # function for updating learning rate\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            # every time we multiply our learning rate (for example 1) with smaller and smaller float (<=1)\n",
        "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    def update_params(self, layer):\n",
        "        # if we use momentum\n",
        "        if not hasattr(layer, 'weight_chache'):\n",
        "            layer.weight_chache = np.zeros_like(layer.weights)\n",
        "            layer.bias_chache = np.zeros_like(layer.biases)\n",
        "\n",
        "        layer.weight_chache += layer.dweights ** 2\n",
        "        layer.bias_chache += layer.dbiases ** 2\n",
        "\n",
        "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_chache) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_chache) + self.epsilon)\n",
        "\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PaXXjBCxbse_"
      },
      "outputs": [],
      "source": [
        "class Optimizer_RMSprop():\n",
        "    def __init__(self, learning_rate=0.001, decay=0, epsilon=1e-7):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    # function for updating learning rate\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            # every time we multiply our learning rate (for example 1) with smaller and smaller float (<=1)\n",
        "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    def update_params(self, layer):\n",
        "        # if we use momentum\n",
        "        if not hasattr(layer, 'weight_chache'):\n",
        "            layer.weight_chache = np.zeros_like(layer.weights)\n",
        "            layer.bias_chache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # layer.weight_chache = self.rho * layer.weight_chache + (1 - self.rho) * layer.dweights ** 2\n",
        "        # layer.bias_chache = self.rho * layer.bias_chache + (1 - self.rho) * layer.dbiases ** 2\n",
        "\n",
        "        layer.weight_chache = self.rho * layer.weight_chache + \\\n",
        "                            (1 - self.rho) * layer.dweights**2\n",
        "        layer.bias_chache = self.rho * layer.bias_chache + \\\n",
        "                            (1 - self.rho) * layer.dbiases**2\n",
        "\n",
        "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_chache) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_chache) + self.epsilon)\n",
        "\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Ftz8T50ebsfO"
      },
      "outputs": [],
      "source": [
        "class Optimizer_Adam():\n",
        "    def __init__(self, learning_rate=0.001, decay=0, epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.beta_1 = beta_1\n",
        "        self.beta_2 = beta_2\n",
        "\n",
        "    # function for updating learning rate\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            # every time we multiply our learning rate (for example 1) with smaller and smaller float (<=1)\n",
        "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    def update_params(self, layer):\n",
        "        # if we use momentum\n",
        "        if not hasattr(layer, 'weight_chache'):\n",
        "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "            layer.weight_chache = np.zeros_like(layer.weights)\n",
        "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "            layer.bias_chache = np.zeros_like(layer.biases)\n",
        "\n",
        "\n",
        "        # update momentums\n",
        "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
        "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
        "\n",
        "        # correct momentums\n",
        "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1**(self.iterations + 1))\n",
        "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1**(self.iterations + 1))\n",
        "\n",
        "        # update chache\n",
        "        layer.weight_chache = self.beta_2 * layer.weight_chache + (1 - self.beta_2) * layer.dweights**2\n",
        "        layer.bias_chache = self.beta_2 * layer.bias_chache + (1 - self.beta_2) * layer.dbiases**2\n",
        "\n",
        "        # correct chache\n",
        "        weight_chache_corrected = layer.weight_chache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "        bias_chache_corrected = layer.bias_chache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "\n",
        "\n",
        "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_chache_corrected) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_chache_corrected) + self.epsilon)\n",
        "\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-AnfTdzJbsfX"
      },
      "outputs": [],
      "source": [
        "# Create model structure\n",
        "dense1 = Layer_Dense(784, 10)\n",
        "activation1 = Activation_ReLU()\n",
        "dense2 = Layer_Dense(10, 10)\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "optimizer = Optimizer_Adam(decay=5e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "sa0iMteKbsfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acf38499-b5b9-4659-ae20-40f722d00e63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, acc: 0.938, loss: 0.223, lr: 0.0009869232667160128\n",
            "epoch: 1, acc: 0.938, loss: 0.223, lr: 0.0009868745682423763\n",
            "epoch: 2, acc: 0.938, loss: 0.223, lr: 0.0009868258745744313\n",
            "epoch: 3, acc: 0.938, loss: 0.223, lr: 0.0009867771857114663\n",
            "epoch: 4, acc: 0.938, loss: 0.223, lr: 0.0009867285016527702\n",
            "epoch: 5, acc: 0.938, loss: 0.222, lr: 0.0009866798223976318\n",
            "epoch: 6, acc: 0.938, loss: 0.222, lr: 0.0009866311479453406\n",
            "epoch: 7, acc: 0.938, loss: 0.222, lr: 0.0009865824782951855\n",
            "epoch: 8, acc: 0.938, loss: 0.222, lr: 0.000986533813446456\n",
            "epoch: 9, acc: 0.938, loss: 0.222, lr: 0.0009864851533984414\n",
            "epoch: 10, acc: 0.938, loss: 0.221, lr: 0.0009864364981504318\n",
            "epoch: 11, acc: 0.938, loss: 0.221, lr: 0.0009863878477017164\n",
            "epoch: 12, acc: 0.938, loss: 0.221, lr: 0.0009863392020515857\n",
            "epoch: 13, acc: 0.938, loss: 0.221, lr: 0.0009862905611993293\n",
            "epoch: 14, acc: 0.938, loss: 0.221, lr: 0.000986241925144238\n",
            "epoch: 15, acc: 0.938, loss: 0.221, lr: 0.0009861932938856016\n",
            "epoch: 16, acc: 0.938, loss: 0.220, lr: 0.000986144667422711\n",
            "epoch: 17, acc: 0.938, loss: 0.220, lr: 0.0009860960457548566\n",
            "epoch: 18, acc: 0.938, loss: 0.220, lr: 0.0009860474288813292\n",
            "epoch: 19, acc: 0.938, loss: 0.220, lr: 0.00098599881680142\n",
            "epoch: 20, acc: 0.938, loss: 0.220, lr: 0.0009859502095144195\n",
            "epoch: 21, acc: 0.938, loss: 0.220, lr: 0.0009859016070196194\n",
            "epoch: 22, acc: 0.938, loss: 0.219, lr: 0.0009858530093163108\n",
            "epoch: 23, acc: 0.939, loss: 0.219, lr: 0.0009858044164037854\n",
            "epoch: 24, acc: 0.939, loss: 0.219, lr: 0.0009857558282813346\n",
            "epoch: 25, acc: 0.939, loss: 0.219, lr: 0.0009857072449482504\n",
            "epoch: 26, acc: 0.939, loss: 0.219, lr: 0.0009856586664038242\n",
            "epoch: 27, acc: 0.939, loss: 0.219, lr: 0.0009856100926473488\n",
            "epoch: 28, acc: 0.939, loss: 0.218, lr: 0.0009855615236781157\n",
            "epoch: 29, acc: 0.939, loss: 0.218, lr: 0.0009855129594954174\n",
            "epoch: 30, acc: 0.939, loss: 0.218, lr: 0.0009854644000985464\n",
            "epoch: 31, acc: 0.939, loss: 0.218, lr: 0.0009854158454867955\n",
            "epoch: 32, acc: 0.939, loss: 0.218, lr: 0.000985367295659457\n",
            "epoch: 33, acc: 0.939, loss: 0.218, lr: 0.0009853187506158243\n",
            "epoch: 34, acc: 0.939, loss: 0.218, lr: 0.00098527021035519\n",
            "epoch: 35, acc: 0.939, loss: 0.217, lr: 0.0009852216748768474\n",
            "epoch: 36, acc: 0.939, loss: 0.217, lr: 0.0009851731441800897\n",
            "epoch: 37, acc: 0.939, loss: 0.217, lr: 0.0009851246182642104\n",
            "epoch: 38, acc: 0.939, loss: 0.217, lr: 0.0009850760971285033\n",
            "epoch: 39, acc: 0.940, loss: 0.217, lr: 0.0009850275807722615\n",
            "epoch: 40, acc: 0.940, loss: 0.217, lr: 0.0009849790691947797\n",
            "epoch: 41, acc: 0.940, loss: 0.217, lr: 0.000984930562395351\n",
            "epoch: 42, acc: 0.940, loss: 0.216, lr: 0.0009848820603732703\n",
            "epoch: 43, acc: 0.940, loss: 0.216, lr: 0.0009848335631278313\n",
            "epoch: 44, acc: 0.940, loss: 0.216, lr: 0.000984785070658329\n",
            "epoch: 45, acc: 0.940, loss: 0.216, lr: 0.0009847365829640572\n",
            "epoch: 46, acc: 0.940, loss: 0.216, lr: 0.000984688100044311\n",
            "epoch: 47, acc: 0.940, loss: 0.216, lr: 0.0009846396218983853\n",
            "epoch: 48, acc: 0.940, loss: 0.215, lr: 0.0009845911485255748\n",
            "epoch: 49, acc: 0.940, loss: 0.215, lr: 0.0009845426799251747\n",
            "epoch: 50, acc: 0.940, loss: 0.215, lr: 0.0009844942160964806\n",
            "epoch: 51, acc: 0.940, loss: 0.215, lr: 0.0009844457570387872\n",
            "epoch: 52, acc: 0.940, loss: 0.215, lr: 0.0009843973027513907\n",
            "epoch: 53, acc: 0.940, loss: 0.215, lr: 0.0009843488532335861\n",
            "epoch: 54, acc: 0.940, loss: 0.215, lr: 0.0009843004084846697\n",
            "epoch: 55, acc: 0.940, loss: 0.214, lr: 0.000984251968503937\n",
            "epoch: 56, acc: 0.940, loss: 0.214, lr: 0.0009842035332906847\n",
            "epoch: 57, acc: 0.940, loss: 0.214, lr: 0.0009841551028442082\n",
            "epoch: 58, acc: 0.940, loss: 0.214, lr: 0.0009841066771638044\n",
            "epoch: 59, acc: 0.940, loss: 0.214, lr: 0.00098405825624877\n",
            "epoch: 60, acc: 0.940, loss: 0.214, lr: 0.0009840098400984009\n",
            "epoch: 61, acc: 0.940, loss: 0.214, lr: 0.0009839614287119945\n",
            "epoch: 62, acc: 0.940, loss: 0.214, lr: 0.0009839130220888473\n",
            "epoch: 63, acc: 0.940, loss: 0.213, lr: 0.0009838646202282567\n",
            "epoch: 64, acc: 0.940, loss: 0.213, lr: 0.0009838162231295194\n",
            "epoch: 65, acc: 0.940, loss: 0.213, lr: 0.0009837678307919333\n",
            "epoch: 66, acc: 0.940, loss: 0.213, lr: 0.000983719443214795\n",
            "epoch: 67, acc: 0.940, loss: 0.213, lr: 0.0009836710603974032\n",
            "epoch: 68, acc: 0.940, loss: 0.213, lr: 0.0009836226823390548\n",
            "epoch: 69, acc: 0.941, loss: 0.213, lr: 0.000983574309039048\n",
            "epoch: 70, acc: 0.940, loss: 0.212, lr: 0.0009835259404966806\n",
            "epoch: 71, acc: 0.941, loss: 0.212, lr: 0.000983477576711251\n",
            "epoch: 72, acc: 0.940, loss: 0.212, lr: 0.0009834292176820573\n",
            "epoch: 73, acc: 0.941, loss: 0.212, lr: 0.0009833808634083982\n",
            "epoch: 74, acc: 0.940, loss: 0.212, lr: 0.0009833325138895717\n",
            "epoch: 75, acc: 0.941, loss: 0.212, lr: 0.0009832841691248771\n",
            "epoch: 76, acc: 0.940, loss: 0.212, lr: 0.0009832358291136129\n",
            "epoch: 77, acc: 0.941, loss: 0.212, lr: 0.0009831874938550783\n",
            "epoch: 78, acc: 0.941, loss: 0.212, lr: 0.000983139163348572\n",
            "epoch: 79, acc: 0.941, loss: 0.211, lr: 0.0009830908375933936\n",
            "epoch: 80, acc: 0.941, loss: 0.211, lr: 0.0009830425165888424\n",
            "epoch: 81, acc: 0.941, loss: 0.211, lr: 0.000982994200334218\n",
            "epoch: 82, acc: 0.941, loss: 0.211, lr: 0.00098294588882882\n",
            "epoch: 83, acc: 0.941, loss: 0.211, lr: 0.000982897582071948\n",
            "epoch: 84, acc: 0.941, loss: 0.211, lr: 0.0009828492800629024\n",
            "epoch: 85, acc: 0.941, loss: 0.211, lr: 0.0009828009828009828\n",
            "epoch: 86, acc: 0.941, loss: 0.211, lr: 0.0009827526902854897\n",
            "epoch: 87, acc: 0.941, loss: 0.211, lr: 0.0009827044025157233\n",
            "epoch: 88, acc: 0.941, loss: 0.210, lr: 0.0009826561194909843\n",
            "epoch: 89, acc: 0.941, loss: 0.210, lr: 0.0009826078412105727\n",
            "epoch: 90, acc: 0.941, loss: 0.210, lr: 0.0009825595676737904\n",
            "epoch: 91, acc: 0.941, loss: 0.210, lr: 0.000982511298879937\n",
            "epoch: 92, acc: 0.941, loss: 0.210, lr: 0.0009824630348283148\n",
            "epoch: 93, acc: 0.941, loss: 0.210, lr: 0.0009824147755182239\n",
            "epoch: 94, acc: 0.942, loss: 0.210, lr: 0.0009823665209489662\n",
            "epoch: 95, acc: 0.942, loss: 0.209, lr: 0.0009823182711198428\n",
            "epoch: 96, acc: 0.942, loss: 0.209, lr: 0.0009822700260301558\n",
            "epoch: 97, acc: 0.942, loss: 0.209, lr: 0.0009822217856792063\n",
            "epoch: 98, acc: 0.942, loss: 0.209, lr: 0.0009821735500662968\n",
            "epoch: 99, acc: 0.942, loss: 0.209, lr: 0.0009821253191907287\n",
            "epoch: 100, acc: 0.941, loss: 0.209, lr: 0.0009820770930518046\n",
            "epoch: 101, acc: 0.942, loss: 0.209, lr: 0.0009820288716488265\n",
            "epoch: 102, acc: 0.941, loss: 0.209, lr: 0.0009819806549810968\n",
            "epoch: 103, acc: 0.942, loss: 0.209, lr: 0.0009819324430479185\n",
            "epoch: 104, acc: 0.941, loss: 0.209, lr: 0.0009818842358485934\n",
            "epoch: 105, acc: 0.942, loss: 0.208, lr: 0.0009818360333824251\n",
            "epoch: 106, acc: 0.942, loss: 0.208, lr: 0.0009817878356487163\n",
            "epoch: 107, acc: 0.942, loss: 0.208, lr: 0.0009817396426467701\n",
            "epoch: 108, acc: 0.942, loss: 0.208, lr: 0.0009816914543758896\n",
            "epoch: 109, acc: 0.942, loss: 0.208, lr: 0.0009816432708353786\n",
            "epoch: 110, acc: 0.942, loss: 0.208, lr: 0.00098159509202454\n",
            "epoch: 111, acc: 0.942, loss: 0.208, lr: 0.0009815469179426776\n",
            "epoch: 112, acc: 0.942, loss: 0.208, lr: 0.0009814987485890956\n",
            "epoch: 113, acc: 0.942, loss: 0.207, lr: 0.0009814505839630975\n",
            "epoch: 114, acc: 0.942, loss: 0.207, lr: 0.0009814024240639874\n",
            "epoch: 115, acc: 0.942, loss: 0.207, lr: 0.0009813542688910698\n",
            "epoch: 116, acc: 0.942, loss: 0.207, lr: 0.0009813061184436485\n",
            "epoch: 117, acc: 0.942, loss: 0.207, lr: 0.0009812579727210284\n",
            "epoch: 118, acc: 0.942, loss: 0.207, lr: 0.0009812098317225138\n",
            "epoch: 119, acc: 0.942, loss: 0.207, lr: 0.0009811616954474097\n",
            "epoch: 120, acc: 0.942, loss: 0.207, lr: 0.000981113563895021\n",
            "epoch: 121, acc: 0.942, loss: 0.207, lr: 0.0009810654370646522\n",
            "epoch: 122, acc: 0.942, loss: 0.207, lr: 0.000981017314955609\n",
            "epoch: 123, acc: 0.942, loss: 0.207, lr: 0.0009809691975671965\n",
            "epoch: 124, acc: 0.942, loss: 0.207, lr: 0.0009809210848987198\n",
            "epoch: 125, acc: 0.942, loss: 0.207, lr: 0.0009808729769494849\n",
            "epoch: 126, acc: 0.942, loss: 0.206, lr: 0.0009808248737187975\n",
            "epoch: 127, acc: 0.942, loss: 0.206, lr: 0.000980776775205963\n",
            "epoch: 128, acc: 0.942, loss: 0.206, lr: 0.000980728681410288\n",
            "epoch: 129, acc: 0.942, loss: 0.206, lr: 0.0009806805923310777\n",
            "epoch: 130, acc: 0.942, loss: 0.206, lr: 0.0009806325079676393\n",
            "epoch: 131, acc: 0.942, loss: 0.206, lr: 0.0009805844283192783\n",
            "epoch: 132, acc: 0.942, loss: 0.206, lr: 0.0009805363533853019\n",
            "epoch: 133, acc: 0.942, loss: 0.206, lr: 0.0009804882831650162\n",
            "epoch: 134, acc: 0.942, loss: 0.205, lr: 0.0009804402176577284\n",
            "epoch: 135, acc: 0.942, loss: 0.205, lr: 0.000980392156862745\n",
            "epoch: 136, acc: 0.942, loss: 0.205, lr: 0.0009803441007793737\n",
            "epoch: 137, acc: 0.942, loss: 0.205, lr: 0.000980296049406921\n",
            "epoch: 138, acc: 0.942, loss: 0.205, lr: 0.0009802480027446942\n",
            "epoch: 139, acc: 0.942, loss: 0.205, lr: 0.0009801999607920015\n",
            "epoch: 140, acc: 0.942, loss: 0.205, lr: 0.00098015192354815\n",
            "epoch: 141, acc: 0.943, loss: 0.205, lr: 0.0009801038910124474\n",
            "epoch: 142, acc: 0.943, loss: 0.205, lr: 0.0009800558631842015\n",
            "epoch: 143, acc: 0.943, loss: 0.204, lr: 0.0009800078400627205\n",
            "epoch: 144, acc: 0.943, loss: 0.204, lr: 0.0009799598216473126\n",
            "epoch: 145, acc: 0.943, loss: 0.204, lr: 0.0009799118079372858\n",
            "epoch: 146, acc: 0.943, loss: 0.204, lr: 0.0009798637989319485\n",
            "epoch: 147, acc: 0.943, loss: 0.204, lr: 0.0009798157946306096\n",
            "epoch: 148, acc: 0.943, loss: 0.204, lr: 0.0009797677950325772\n",
            "epoch: 149, acc: 0.943, loss: 0.204, lr: 0.000979719800137161\n",
            "epoch: 150, acc: 0.943, loss: 0.204, lr: 0.0009796718099436689\n",
            "epoch: 151, acc: 0.943, loss: 0.204, lr: 0.0009796238244514108\n",
            "epoch: 152, acc: 0.943, loss: 0.204, lr: 0.0009795758436596954\n",
            "epoch: 153, acc: 0.943, loss: 0.204, lr: 0.0009795278675678325\n",
            "epoch: 154, acc: 0.943, loss: 0.203, lr: 0.000979479896175131\n",
            "epoch: 155, acc: 0.943, loss: 0.203, lr: 0.0009794319294809011\n",
            "epoch: 156, acc: 0.943, loss: 0.203, lr: 0.0009793839674844523\n",
            "epoch: 157, acc: 0.943, loss: 0.203, lr: 0.0009793360101850947\n",
            "epoch: 158, acc: 0.943, loss: 0.203, lr: 0.0009792880575821379\n",
            "epoch: 159, acc: 0.943, loss: 0.203, lr: 0.0009792401096748921\n",
            "epoch: 160, acc: 0.943, loss: 0.203, lr: 0.0009791921664626683\n",
            "epoch: 161, acc: 0.943, loss: 0.203, lr: 0.0009791442279447763\n",
            "epoch: 162, acc: 0.943, loss: 0.203, lr: 0.0009790962941205268\n",
            "epoch: 163, acc: 0.943, loss: 0.203, lr: 0.0009790483649892304\n",
            "epoch: 164, acc: 0.943, loss: 0.203, lr: 0.0009790004405501984\n",
            "epoch: 165, acc: 0.943, loss: 0.203, lr: 0.000978952520802741\n",
            "epoch: 166, acc: 0.943, loss: 0.203, lr: 0.00097890460574617\n",
            "epoch: 167, acc: 0.943, loss: 0.203, lr: 0.0009788566953797965\n",
            "epoch: 168, acc: 0.943, loss: 0.203, lr: 0.0009788087897029316\n",
            "epoch: 169, acc: 0.943, loss: 0.202, lr: 0.0009787608887148868\n",
            "epoch: 170, acc: 0.943, loss: 0.202, lr: 0.0009787129924149743\n",
            "epoch: 171, acc: 0.943, loss: 0.202, lr: 0.0009786651008025053\n",
            "epoch: 172, acc: 0.943, loss: 0.202, lr: 0.0009786172138767921\n",
            "epoch: 173, acc: 0.943, loss: 0.202, lr: 0.0009785693316371464\n",
            "epoch: 174, acc: 0.943, loss: 0.202, lr: 0.0009785214540828809\n",
            "epoch: 175, acc: 0.943, loss: 0.202, lr: 0.0009784735812133072\n",
            "epoch: 176, acc: 0.943, loss: 0.202, lr: 0.0009784257130277386\n",
            "epoch: 177, acc: 0.943, loss: 0.202, lr: 0.0009783778495254867\n",
            "epoch: 178, acc: 0.943, loss: 0.201, lr: 0.0009783299907058652\n",
            "epoch: 179, acc: 0.944, loss: 0.201, lr: 0.0009782821365681863\n",
            "epoch: 180, acc: 0.944, loss: 0.201, lr: 0.0009782342871117633\n",
            "epoch: 181, acc: 0.944, loss: 0.201, lr: 0.0009781864423359092\n",
            "epoch: 182, acc: 0.944, loss: 0.201, lr: 0.0009781386022399373\n",
            "epoch: 183, acc: 0.944, loss: 0.201, lr: 0.0009780907668231612\n",
            "epoch: 184, acc: 0.944, loss: 0.201, lr: 0.000978042936084894\n",
            "epoch: 185, acc: 0.944, loss: 0.201, lr: 0.00097799511002445\n",
            "epoch: 186, acc: 0.943, loss: 0.201, lr: 0.0009779472886411423\n",
            "epoch: 187, acc: 0.944, loss: 0.201, lr: 0.0009778994719342852\n",
            "epoch: 188, acc: 0.943, loss: 0.201, lr: 0.0009778516599031927\n",
            "epoch: 189, acc: 0.943, loss: 0.201, lr: 0.000977803852547179\n",
            "epoch: 190, acc: 0.944, loss: 0.201, lr: 0.0009777560498655585\n",
            "epoch: 191, acc: 0.944, loss: 0.201, lr: 0.0009777082518576457\n",
            "epoch: 192, acc: 0.944, loss: 0.200, lr: 0.000977660458522755\n",
            "epoch: 193, acc: 0.944, loss: 0.200, lr: 0.0009776126698602015\n",
            "epoch: 194, acc: 0.944, loss: 0.200, lr: 0.0009775648858692996\n",
            "epoch: 195, acc: 0.944, loss: 0.200, lr: 0.0009775171065493648\n",
            "epoch: 196, acc: 0.944, loss: 0.200, lr: 0.0009774693318997116\n",
            "epoch: 197, acc: 0.944, loss: 0.200, lr: 0.000977421561919656\n",
            "epoch: 198, acc: 0.944, loss: 0.200, lr: 0.0009773737966085128\n",
            "epoch: 199, acc: 0.944, loss: 0.200, lr: 0.000977326035965598\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(200):\n",
        "    # Forward pass\n",
        "    dense1.forward(X_train)\n",
        "    activation1.forward(dense1.output)\n",
        "    dense2.forward(activation1.output)\n",
        "    loss = loss_activation.forward(dense2.output, y_train)\n",
        "\n",
        "    predictions = np.argmax(loss_activation.output, axis=1)\n",
        "    if len(y_train.shape) == 2:\n",
        "        y_train = np.argmax(y_train, axis=1)\n",
        "\n",
        "    accuracy = np.mean(predictions==y_train)\n",
        "\n",
        "    # if epoch % 100 == 0:\n",
        "    print(f'epoch: {epoch}, ' + f'acc: {accuracy:.3f}, ' + f'loss: {loss:.3f}, ' + f'lr: {optimizer.current_learning_rate}')\n",
        "\n",
        "    # Backward pass\n",
        "    loss_activation.backward(loss_activation.output, y_train)\n",
        "    dense2.backward(loss_activation.dinputs)\n",
        "    activation1.backward(dense2.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "\n",
        "    # Optimizing\n",
        "    optimizer.pre_update_params()\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.post_update_params()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}