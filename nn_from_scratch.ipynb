{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5iQY-SW3B7pF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mnist import MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mB4wzybMbseq"
      },
      "outputs": [],
      "source": [
        "# Reading the dataset\n",
        "mndata = MNIST('samples/')\n",
        "\n",
        "X_train, y_train = mndata.load_training()\n",
        "X_test, y_test = mndata.load_testing()\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5WbFgBE0CJmP"
      },
      "outputs": [],
      "source": [
        "class Layer_Dense():\n",
        "    def __init__(self, n_inputs, n_neurons):\n",
        "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "        self.inputs = inputs # we want to remember what our inputs are\n",
        "\n",
        "    def backward(self, dvalues):\n",
        "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "        # for easier understanding, we can calculate dbiases with\n",
        "        # np.dot(dvalues, np.array([1, 1, 1]).T) (second argument is derivative of z with respect to biases for all samples)\n",
        "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "        self.dinputs = np.dot(dvalues, self.weights.T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VcCsDY4vCRR1"
      },
      "outputs": [],
      "source": [
        "class Activation_ReLU():\n",
        "    def forward(self, inputs):\n",
        "        self.output = np.maximum(0, inputs)\n",
        "        self.inputs = inputs\n",
        "\n",
        "    def backward(self, dvalues):\n",
        "        \"\"\"\n",
        "        the same but with multiplication and more clearly:\n",
        "\n",
        "        drelu_dinputs[inputs <= 0] = 0\n",
        "        drelu_dinputs[inputs > 0] = 1\n",
        "        dinputs = dvalues * drelu_dinputs\n",
        "        \"\"\"\n",
        "        self.dinputs = dvalues.copy()\n",
        "        self.dinputs[self.inputs <= 0] = 0\n",
        "\n",
        "class Activation_Softmax():\n",
        "    def forward(self, inputs):\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "        probabilities = exp_values / exp_values.sum(axis=1, keepdims=True)\n",
        "        self.output = probabilities\n",
        "\n",
        "        # Backward pass\n",
        "    def backward(self, dvalues):\n",
        "        # Create uninitialized array\n",
        "        self.dinputs = np.empty_like(dvalues)\n",
        "        # Enumerate outputs and gradients\n",
        "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
        "            # Flatten output array\n",
        "            single_output = single_output.reshape(-1, 1)\n",
        "            # Calculate Jacobian matrix of the output and\n",
        "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
        "\n",
        "            # Calculate sample-wise gradient\n",
        "            # and add it to the array of sample gradients\n",
        "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ZTfgoZtRCYyi"
      },
      "outputs": [],
      "source": [
        "class Loss():\n",
        "    def calculate(self, output, y):\n",
        "        sample_losses = self.forward(output, y)\n",
        "        data_loss = np.mean(sample_losses)\n",
        "        return data_loss\n",
        "\n",
        "class Loss_CategoricalCrossentropy(Loss):\n",
        "    def forward(self, y_pred, y_true):\n",
        "        self.y_pred = y_pred\n",
        "        self.y_true = y_true\n",
        "\n",
        "        samples = len(y_pred)\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        if len(y_true.shape) == 1:\n",
        "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
        "        elif len(y_true.shape) == 2:\n",
        "            correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n",
        "\n",
        "        return -np.log(correct_confidences)\n",
        "\n",
        "    def backward(self, dvalues, y_true):\n",
        "        # dvalues actually is a matrix which contains predictions (samples x predictions)\n",
        "        samples = len(dvalues)\n",
        "        # Number of labels in every sample\n",
        "        # We'll use the first sample to count them\n",
        "        labels = dvalues[0]\n",
        "        # If labels are sparse, turn them into one-hot vector\n",
        "        if len(y_true.shape) == 1:\n",
        "            y_true = np.eye(labels)[y_true]\n",
        "\n",
        "        dinputs = - y_true / dvalues\n",
        "\n",
        "        # this normalization step ensures that the gradients are averaged across all samples\n",
        "        dinputs = dinputs / samples\n",
        "\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
        "    def __init__(self):\n",
        "        self.activation = Activation_Softmax()\n",
        "        self.loss = Loss_CategoricalCrossentropy()\n",
        "\n",
        "    def forward(self, inputs, y_true):\n",
        "        # Output layer's activation function\n",
        "        self.activation.forward(inputs)\n",
        "        # Set the output\n",
        "        self.output = self.activation.output\n",
        "        # Calculate and return loss value\n",
        "        return self.loss.calculate(self.output, y_true)\n",
        "\n",
        "        # Backward pass\n",
        "    def backward(self, dvalues, y_true):\n",
        "        # derivative of softmax + loss is (y_hat - y_true)\n",
        "        # Number of samples\n",
        "        samples = len(dvalues)\n",
        "\n",
        "        if len(y_true.shape) == 2:\n",
        "            y_true = np.argmax(y_true, axis=1)\n",
        "        # Copy so we can safely modify\n",
        "        self.dinputs = dvalues.copy()\n",
        "        # Calculate gradient\n",
        "        self.dinputs[range(samples), y_true] -= 1\n",
        "        # Normalize gradient\n",
        "        self.dinputs = self.dinputs / samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JMw0tox_CgYv"
      },
      "outputs": [],
      "source": [
        "class Optimizer_SGD():\n",
        "    def __init__(self, learning_rate=1.0, decay=.0, momentum=0.):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.momentum = momentum\n",
        "        self.iterations = 0\n",
        "\n",
        "    # function for updating learning rate\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            # every time we multiply our learning rate (for example 1) with smaller and smaller float (<=1)\n",
        "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    def update_params(self, layer):\n",
        "        # if we use momentum\n",
        "        if self.momentum:\n",
        "            # If layer does not contain momentum arrays, create them\n",
        "            if not hasattr(layer, 'weight_momentums'):\n",
        "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "\n",
        "            # while updating our wwights we take into count our prevous gradients\n",
        "            # momentum walue determines how much we want to keep out prevous gradients\n",
        "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
        "            layer.weight_momentums = weight_updates\n",
        "\n",
        "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
        "            layer.bias_momentums = bias_updates\n",
        "\n",
        "        else: # If we dont use momentum\n",
        "            weight_updates = -self.learning_rate * layer.dweights\n",
        "            bias_updates = -self.learning_rate * layer.dbiases\n",
        "\n",
        "        layer.weights += weight_updates\n",
        "        layer.biases += bias_updates\n",
        "\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lbtm-K19bse9"
      },
      "outputs": [],
      "source": [
        "class Optimizer_AdaGrad():\n",
        "    def __init__(self, learning_rate=1.0, decay=.0, epsilon=1e-7):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    # function for updating learning rate\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            # every time we multiply our learning rate (for example 1) with smaller and smaller float (<=1)\n",
        "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    def update_params(self, layer):\n",
        "        # if we use momentum\n",
        "        if not hasattr(layer, 'weight_chache'):\n",
        "            layer.weight_chache = np.zeros_like(layer.weights)\n",
        "            layer.bias_chache = np.zeros_like(layer.biases)\n",
        "\n",
        "        layer.weight_chache += layer.dweights ** 2\n",
        "        layer.bias_chache += layer.dbiases ** 2\n",
        "\n",
        "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_chache) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_chache) + self.epsilon)\n",
        "\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PaXXjBCxbse_"
      },
      "outputs": [],
      "source": [
        "class Optimizer_RMSprop():\n",
        "    def __init__(self, learning_rate=0.001, decay=0, epsilon=1e-7):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    # function for updating learning rate\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            # every time we multiply our learning rate (for example 1) with smaller and smaller float (<=1)\n",
        "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    def update_params(self, layer):\n",
        "        # if we use momentum\n",
        "        if not hasattr(layer, 'weight_chache'):\n",
        "            layer.weight_chache = np.zeros_like(layer.weights)\n",
        "            layer.bias_chache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # layer.weight_chache = self.rho * layer.weight_chache + (1 - self.rho) * layer.dweights ** 2\n",
        "        # layer.bias_chache = self.rho * layer.bias_chache + (1 - self.rho) * layer.dbiases ** 2\n",
        "\n",
        "        layer.weight_chache = self.rho * layer.weight_chache + \\\n",
        "                            (1 - self.rho) * layer.dweights**2\n",
        "        layer.bias_chache = self.rho * layer.bias_chache + \\\n",
        "                            (1 - self.rho) * layer.dbiases**2\n",
        "\n",
        "        layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_chache) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_chache) + self.epsilon)\n",
        "\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Ftz8T50ebsfO"
      },
      "outputs": [],
      "source": [
        "class Optimizer_Adam():\n",
        "    def __init__(self, learning_rate=0.001, decay=0, epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.current_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.iterations = 0\n",
        "        self.epsilon = epsilon\n",
        "        self.beta_1 = beta_1\n",
        "        self.beta_2 = beta_2\n",
        "\n",
        "    # function for updating learning rate\n",
        "    def pre_update_params(self):\n",
        "        if self.decay:\n",
        "            # every time we multiply our learning rate (for example 1) with smaller and smaller float (<=1)\n",
        "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "    def update_params(self, layer):\n",
        "        # if we use momentum\n",
        "        if not hasattr(layer, 'weight_chache'):\n",
        "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "            layer.weight_chache = np.zeros_like(layer.weights)\n",
        "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "            layer.bias_chache = np.zeros_like(layer.biases)\n",
        "\n",
        "\n",
        "        # update momentums\n",
        "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
        "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
        "\n",
        "        # correct momentums\n",
        "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1**(self.iterations + 1))\n",
        "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1**(self.iterations + 1))\n",
        "\n",
        "        # update chache\n",
        "        layer.weight_chache = self.beta_2 * layer.weight_chache + (1 - self.beta_2) * layer.dweights**2\n",
        "        layer.bias_chache = self.beta_2 * layer.bias_chache + (1 - self.beta_2) * layer.dbiases**2\n",
        "\n",
        "        # correct chache\n",
        "        weight_chache_corrected = layer.weight_chache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "        bias_chache_corrected = layer.bias_chache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "\n",
        "\n",
        "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_chache_corrected) + self.epsilon)\n",
        "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_chache_corrected) + self.epsilon)\n",
        "\n",
        "    def post_update_params(self):\n",
        "        self.iterations += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-AnfTdzJbsfX"
      },
      "outputs": [],
      "source": [
        "# Create model structure\n",
        "dense1 = Layer_Dense(784, 10)\n",
        "activation1 = Activation_ReLU()\n",
        "dense2 = Layer_Dense(10, 10)\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "optimizer = Optimizer_Adam(decay=5e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa0iMteKbsfa",
        "outputId": "bbee1c46-f354-4802-ba93-cb1911eb771b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 0, acc: 0.107, loss: 2.463, lr: 0.001\n",
            "epoch: 1, acc: 0.191, loss: 2.195, lr: 0.001\n",
            "epoch: 2, acc: 0.261, loss: 2.061, lr: 0.000999950002499875\n",
            "epoch: 3, acc: 0.294, loss: 1.958, lr: 0.000999900009999\n",
            "epoch: 4, acc: 0.316, loss: 1.871, lr: 0.0009998500224966255\n",
            "epoch: 5, acc: 0.358, loss: 1.778, lr: 0.0009998000399920016\n",
            "epoch: 6, acc: 0.429, loss: 1.677, lr: 0.0009997500624843788\n",
            "epoch: 7, acc: 0.488, loss: 1.571, lr: 0.000999700089973008\n",
            "epoch: 8, acc: 0.521, loss: 1.474, lr: 0.0009996501224571398\n",
            "epoch: 9, acc: 0.533, loss: 1.394, lr: 0.0009996001599360256\n",
            "epoch: 10, acc: 0.544, loss: 1.332, lr: 0.0009995502024089159\n",
            "epoch: 11, acc: 0.558, loss: 1.275, lr: 0.0009995002498750624\n",
            "epoch: 12, acc: 0.576, loss: 1.212, lr: 0.0009994503023337165\n",
            "epoch: 13, acc: 0.598, loss: 1.148, lr: 0.0009994003597841295\n",
            "epoch: 14, acc: 0.623, loss: 1.093, lr: 0.0009993504222255533\n",
            "epoch: 15, acc: 0.643, loss: 1.044, lr: 0.0009993004896572402\n",
            "epoch: 16, acc: 0.662, loss: 0.998, lr: 0.0009992505620784412\n",
            "epoch: 17, acc: 0.683, loss: 0.956, lr: 0.0009992006394884093\n",
            "epoch: 18, acc: 0.700, loss: 0.917, lr: 0.0009991507218863964\n",
            "epoch: 19, acc: 0.714, loss: 0.876, lr: 0.0009991008092716555\n",
            "epoch: 20, acc: 0.726, loss: 0.834, lr: 0.0009990509016434388\n",
            "epoch: 21, acc: 0.740, loss: 0.791, lr: 0.0009990009990009992\n",
            "epoch: 22, acc: 0.753, loss: 0.751, lr: 0.0009989511013435892\n",
            "epoch: 23, acc: 0.762, loss: 0.718, lr: 0.0009989012086704624\n",
            "epoch: 24, acc: 0.770, loss: 0.689, lr: 0.0009988513209808721\n",
            "epoch: 25, acc: 0.778, loss: 0.662, lr: 0.000998801438274071\n",
            "epoch: 26, acc: 0.790, loss: 0.635, lr: 0.0009987515605493133\n",
            "epoch: 27, acc: 0.802, loss: 0.609, lr: 0.0009987016878058523\n",
            "epoch: 28, acc: 0.815, loss: 0.586, lr: 0.0009986518200429421\n",
            "epoch: 29, acc: 0.824, loss: 0.566, lr: 0.000998601957259836\n",
            "epoch: 30, acc: 0.829, loss: 0.549, lr: 0.000998552099455789\n",
            "epoch: 31, acc: 0.837, loss: 0.531, lr: 0.0009985022466300548\n",
            "epoch: 32, acc: 0.844, loss: 0.515, lr: 0.0009984523987818883\n",
            "epoch: 33, acc: 0.849, loss: 0.500, lr: 0.000998402555910543\n",
            "epoch: 34, acc: 0.853, loss: 0.486, lr: 0.000998352718015275\n",
            "epoch: 35, acc: 0.859, loss: 0.473, lr: 0.000998302885095338\n",
            "epoch: 36, acc: 0.863, loss: 0.460, lr: 0.0009982530571499876\n",
            "epoch: 37, acc: 0.867, loss: 0.448, lr: 0.0009982032341784787\n",
            "epoch: 38, acc: 0.871, loss: 0.437, lr: 0.0009981534161800669\n",
            "epoch: 39, acc: 0.874, loss: 0.427, lr: 0.0009981036031540074\n",
            "epoch: 40, acc: 0.878, loss: 0.417, lr: 0.000998053795099556\n",
            "epoch: 41, acc: 0.881, loss: 0.407, lr: 0.000998003992015968\n",
            "epoch: 42, acc: 0.884, loss: 0.398, lr: 0.0009979541939024999\n",
            "epoch: 43, acc: 0.886, loss: 0.391, lr: 0.0009979044007584073\n",
            "epoch: 44, acc: 0.889, loss: 0.384, lr: 0.0009978546125829467\n",
            "epoch: 45, acc: 0.891, loss: 0.377, lr: 0.0009978048293753743\n",
            "epoch: 46, acc: 0.893, loss: 0.370, lr: 0.0009977550511349464\n",
            "epoch: 47, acc: 0.895, loss: 0.364, lr: 0.0009977052778609198\n",
            "epoch: 48, acc: 0.896, loss: 0.359, lr: 0.0009976555095525515\n",
            "epoch: 49, acc: 0.898, loss: 0.354, lr: 0.0009976057462090984\n",
            "epoch: 50, acc: 0.900, loss: 0.349, lr: 0.0009975559878298169\n",
            "epoch: 51, acc: 0.901, loss: 0.344, lr: 0.0009975062344139652\n",
            "epoch: 52, acc: 0.902, loss: 0.340, lr: 0.0009974564859608\n",
            "epoch: 53, acc: 0.904, loss: 0.337, lr: 0.0009974067424695793\n",
            "epoch: 54, acc: 0.904, loss: 0.333, lr: 0.0009973570039395602\n",
            "epoch: 55, acc: 0.905, loss: 0.330, lr: 0.0009973072703700011\n",
            "epoch: 56, acc: 0.906, loss: 0.327, lr: 0.0009972575417601596\n",
            "epoch: 57, acc: 0.906, loss: 0.324, lr: 0.000997207818109294\n",
            "epoch: 58, acc: 0.907, loss: 0.321, lr: 0.0009971580994166626\n",
            "epoch: 59, acc: 0.908, loss: 0.319, lr: 0.0009971083856815236\n",
            "epoch: 60, acc: 0.909, loss: 0.316, lr: 0.0009970586769031359\n",
            "epoch: 61, acc: 0.910, loss: 0.314, lr: 0.0009970089730807579\n",
            "epoch: 62, acc: 0.911, loss: 0.312, lr: 0.0009969592742136485\n",
            "epoch: 63, acc: 0.912, loss: 0.310, lr: 0.0009969095803010666\n",
            "epoch: 64, acc: 0.912, loss: 0.308, lr: 0.0009968598913422718\n",
            "epoch: 65, acc: 0.912, loss: 0.306, lr: 0.000996810207336523\n",
            "epoch: 66, acc: 0.913, loss: 0.304, lr: 0.00099676052828308\n",
            "epoch: 67, acc: 0.913, loss: 0.302, lr: 0.000996710854181202\n",
            "epoch: 68, acc: 0.914, loss: 0.301, lr: 0.000996661185030149\n",
            "epoch: 69, acc: 0.914, loss: 0.299, lr: 0.0009966115208291807\n",
            "epoch: 70, acc: 0.915, loss: 0.297, lr: 0.0009965618615775575\n",
            "epoch: 71, acc: 0.915, loss: 0.296, lr: 0.000996512207274539\n",
            "epoch: 72, acc: 0.915, loss: 0.294, lr: 0.0009964625579193863\n",
            "epoch: 73, acc: 0.916, loss: 0.293, lr: 0.0009964129135113591\n",
            "epoch: 74, acc: 0.916, loss: 0.292, lr: 0.0009963632740497186\n",
            "epoch: 75, acc: 0.917, loss: 0.290, lr: 0.0009963136395337252\n",
            "epoch: 76, acc: 0.917, loss: 0.289, lr: 0.0009962640099626403\n",
            "epoch: 77, acc: 0.918, loss: 0.288, lr: 0.0009962143853357243\n",
            "epoch: 78, acc: 0.918, loss: 0.287, lr: 0.000996164765652239\n",
            "epoch: 79, acc: 0.919, loss: 0.285, lr: 0.0009961151509114453\n",
            "epoch: 80, acc: 0.919, loss: 0.284, lr: 0.0009960655411126052\n",
            "epoch: 81, acc: 0.919, loss: 0.283, lr: 0.00099601593625498\n",
            "epoch: 82, acc: 0.920, loss: 0.282, lr: 0.0009959663363378318\n",
            "epoch: 83, acc: 0.920, loss: 0.281, lr: 0.0009959167413604224\n",
            "epoch: 84, acc: 0.920, loss: 0.280, lr: 0.0009958671513220136\n",
            "epoch: 85, acc: 0.921, loss: 0.279, lr: 0.0009958175662218682\n",
            "epoch: 86, acc: 0.921, loss: 0.278, lr: 0.0009957679860592482\n",
            "epoch: 87, acc: 0.921, loss: 0.277, lr: 0.0009957184108334164\n",
            "epoch: 88, acc: 0.922, loss: 0.276, lr: 0.0009956688405436352\n",
            "epoch: 89, acc: 0.922, loss: 0.275, lr: 0.0009956192751891678\n",
            "epoch: 90, acc: 0.922, loss: 0.274, lr: 0.0009955697147692767\n",
            "epoch: 91, acc: 0.922, loss: 0.273, lr: 0.0009955201592832257\n",
            "epoch: 92, acc: 0.923, loss: 0.272, lr: 0.0009954706087302772\n",
            "epoch: 93, acc: 0.923, loss: 0.271, lr: 0.0009954210631096954\n",
            "epoch: 94, acc: 0.923, loss: 0.271, lr: 0.0009953715224207435\n",
            "epoch: 95, acc: 0.924, loss: 0.270, lr: 0.0009953219866626855\n",
            "epoch: 96, acc: 0.924, loss: 0.269, lr: 0.0009952724558347848\n",
            "epoch: 97, acc: 0.924, loss: 0.268, lr: 0.0009952229299363057\n",
            "epoch: 98, acc: 0.924, loss: 0.268, lr: 0.0009951734089665124\n",
            "epoch: 99, acc: 0.925, loss: 0.267, lr: 0.0009951238929246692\n",
            "epoch: 100, acc: 0.925, loss: 0.266, lr: 0.0009950743818100403\n",
            "epoch: 101, acc: 0.925, loss: 0.265, lr: 0.0009950248756218907\n",
            "epoch: 102, acc: 0.925, loss: 0.265, lr: 0.0009949753743594845\n",
            "epoch: 103, acc: 0.925, loss: 0.264, lr: 0.0009949258780220871\n",
            "epoch: 104, acc: 0.925, loss: 0.263, lr: 0.0009948763866089638\n",
            "epoch: 105, acc: 0.926, loss: 0.262, lr: 0.000994826900119379\n",
            "epoch: 106, acc: 0.926, loss: 0.262, lr: 0.000994777418552599\n",
            "epoch: 107, acc: 0.926, loss: 0.261, lr: 0.0009947279419078882\n",
            "epoch: 108, acc: 0.926, loss: 0.260, lr: 0.000994678470184513\n",
            "epoch: 109, acc: 0.927, loss: 0.260, lr: 0.0009946290033817386\n",
            "epoch: 110, acc: 0.927, loss: 0.259, lr: 0.0009945795414988314\n",
            "epoch: 111, acc: 0.927, loss: 0.258, lr: 0.000994530084535057\n",
            "epoch: 112, acc: 0.927, loss: 0.258, lr: 0.0009944806324896824\n",
            "epoch: 113, acc: 0.928, loss: 0.257, lr: 0.000994431185361973\n",
            "epoch: 114, acc: 0.928, loss: 0.257, lr: 0.0009943817431511959\n",
            "epoch: 115, acc: 0.928, loss: 0.256, lr: 0.0009943323058566173\n",
            "epoch: 116, acc: 0.928, loss: 0.255, lr: 0.0009942828734775045\n",
            "epoch: 117, acc: 0.928, loss: 0.255, lr: 0.0009942334460131238\n",
            "epoch: 118, acc: 0.929, loss: 0.254, lr: 0.000994184023462743\n",
            "epoch: 119, acc: 0.929, loss: 0.253, lr: 0.0009941346058256288\n",
            "epoch: 120, acc: 0.929, loss: 0.253, lr: 0.0009940851931010488\n",
            "epoch: 121, acc: 0.929, loss: 0.252, lr: 0.0009940357852882705\n",
            "epoch: 122, acc: 0.929, loss: 0.252, lr: 0.0009939863823865613\n",
            "epoch: 123, acc: 0.929, loss: 0.251, lr: 0.0009939369843951894\n",
            "epoch: 124, acc: 0.930, loss: 0.250, lr: 0.0009938875913134224\n",
            "epoch: 125, acc: 0.930, loss: 0.250, lr: 0.0009938382031405288\n",
            "epoch: 126, acc: 0.930, loss: 0.249, lr: 0.0009937888198757762\n",
            "epoch: 127, acc: 0.930, loss: 0.248, lr: 0.0009937394415184338\n",
            "epoch: 128, acc: 0.930, loss: 0.248, lr: 0.0009936900680677697\n",
            "epoch: 129, acc: 0.931, loss: 0.247, lr: 0.0009936406995230524\n",
            "epoch: 130, acc: 0.931, loss: 0.247, lr: 0.000993591335883551\n",
            "epoch: 131, acc: 0.931, loss: 0.246, lr: 0.0009935419771485345\n",
            "epoch: 132, acc: 0.931, loss: 0.245, lr: 0.0009934926233172718\n",
            "epoch: 133, acc: 0.931, loss: 0.245, lr: 0.0009934432743890324\n",
            "epoch: 134, acc: 0.931, loss: 0.244, lr: 0.0009933939303630854\n",
            "epoch: 135, acc: 0.932, loss: 0.244, lr: 0.0009933445912387007\n",
            "epoch: 136, acc: 0.932, loss: 0.243, lr: 0.0009932952570151478\n",
            "epoch: 137, acc: 0.932, loss: 0.243, lr: 0.0009932459276916965\n",
            "epoch: 138, acc: 0.932, loss: 0.242, lr: 0.0009931966032676169\n",
            "epoch: 139, acc: 0.933, loss: 0.241, lr: 0.000993147283742179\n",
            "epoch: 140, acc: 0.933, loss: 0.241, lr: 0.0009930979691146532\n",
            "epoch: 141, acc: 0.933, loss: 0.240, lr: 0.00099304865938431\n",
            "epoch: 142, acc: 0.933, loss: 0.240, lr: 0.0009929993545504195\n",
            "epoch: 143, acc: 0.933, loss: 0.239, lr: 0.000992950054612253\n",
            "epoch: 144, acc: 0.934, loss: 0.239, lr: 0.0009929007595690811\n",
            "epoch: 145, acc: 0.934, loss: 0.238, lr: 0.0009928514694201747\n",
            "epoch: 146, acc: 0.934, loss: 0.238, lr: 0.0009928021841648052\n",
            "epoch: 147, acc: 0.934, loss: 0.237, lr: 0.0009927529038022435\n",
            "epoch: 148, acc: 0.934, loss: 0.237, lr: 0.0009927036283317616\n",
            "epoch: 149, acc: 0.934, loss: 0.236, lr: 0.0009926543577526304\n",
            "epoch: 150, acc: 0.935, loss: 0.236, lr: 0.0009926050920641223\n",
            "epoch: 151, acc: 0.935, loss: 0.235, lr: 0.0009925558312655087\n",
            "epoch: 152, acc: 0.935, loss: 0.235, lr: 0.0009925065753560618\n",
            "epoch: 153, acc: 0.935, loss: 0.234, lr: 0.0009924573243350536\n",
            "epoch: 154, acc: 0.935, loss: 0.234, lr: 0.0009924080782017567\n",
            "epoch: 155, acc: 0.935, loss: 0.233, lr: 0.0009923588369554431\n",
            "epoch: 156, acc: 0.936, loss: 0.233, lr: 0.000992309600595386\n",
            "epoch: 157, acc: 0.936, loss: 0.232, lr: 0.0009922603691208572\n",
            "epoch: 158, acc: 0.936, loss: 0.232, lr: 0.0009922111425311308\n",
            "epoch: 159, acc: 0.936, loss: 0.231, lr: 0.0009921619208254787\n",
            "epoch: 160, acc: 0.936, loss: 0.231, lr: 0.0009921127040031748\n",
            "epoch: 161, acc: 0.936, loss: 0.230, lr: 0.000992063492063492\n",
            "epoch: 162, acc: 0.936, loss: 0.230, lr: 0.0009920142850057042\n",
            "epoch: 163, acc: 0.936, loss: 0.230, lr: 0.0009919650828290846\n",
            "epoch: 164, acc: 0.937, loss: 0.229, lr: 0.0009919158855329067\n",
            "epoch: 165, acc: 0.937, loss: 0.229, lr: 0.0009918666931164452\n",
            "epoch: 166, acc: 0.937, loss: 0.228, lr: 0.0009918175055789735\n",
            "epoch: 167, acc: 0.937, loss: 0.228, lr: 0.000991768322919766\n",
            "epoch: 168, acc: 0.937, loss: 0.227, lr: 0.0009917191451380967\n",
            "epoch: 169, acc: 0.937, loss: 0.227, lr: 0.0009916699722332409\n",
            "epoch: 170, acc: 0.938, loss: 0.227, lr: 0.0009916208042044722\n",
            "epoch: 171, acc: 0.938, loss: 0.226, lr: 0.000991571641051066\n",
            "epoch: 172, acc: 0.938, loss: 0.226, lr: 0.0009915224827722969\n",
            "epoch: 173, acc: 0.938, loss: 0.225, lr: 0.00099147332936744\n",
            "epoch: 174, acc: 0.938, loss: 0.225, lr: 0.0009914241808357705\n",
            "epoch: 175, acc: 0.938, loss: 0.224, lr: 0.000991375037176564\n",
            "epoch: 176, acc: 0.938, loss: 0.224, lr: 0.0009913258983890955\n",
            "epoch: 177, acc: 0.938, loss: 0.224, lr: 0.0009912767644726407\n",
            "epoch: 178, acc: 0.938, loss: 0.223, lr: 0.0009912276354264757\n",
            "epoch: 179, acc: 0.938, loss: 0.223, lr: 0.0009911785112498763\n",
            "epoch: 180, acc: 0.939, loss: 0.222, lr: 0.000991129391942118\n",
            "epoch: 181, acc: 0.939, loss: 0.222, lr: 0.000991080277502478\n",
            "epoch: 182, acc: 0.939, loss: 0.222, lr: 0.0009910311679302314\n",
            "epoch: 183, acc: 0.939, loss: 0.221, lr: 0.0009909820632246556\n",
            "epoch: 184, acc: 0.939, loss: 0.221, lr: 0.000990932963385027\n",
            "epoch: 185, acc: 0.939, loss: 0.221, lr: 0.0009908838684106222\n",
            "epoch: 186, acc: 0.939, loss: 0.220, lr: 0.0009908347783007185\n",
            "epoch: 187, acc: 0.939, loss: 0.220, lr: 0.0009907856930545921\n",
            "epoch: 188, acc: 0.939, loss: 0.219, lr: 0.0009907366126715215\n",
            "epoch: 189, acc: 0.940, loss: 0.219, lr: 0.0009906875371507827\n",
            "epoch: 190, acc: 0.940, loss: 0.219, lr: 0.000990638466491654\n",
            "epoch: 191, acc: 0.940, loss: 0.218, lr: 0.0009905894006934125\n",
            "epoch: 192, acc: 0.940, loss: 0.218, lr: 0.0009905403397553365\n",
            "epoch: 193, acc: 0.940, loss: 0.218, lr: 0.0009904912836767037\n",
            "epoch: 194, acc: 0.940, loss: 0.217, lr: 0.000990442232456792\n",
            "epoch: 195, acc: 0.940, loss: 0.217, lr: 0.0009903931860948795\n",
            "epoch: 196, acc: 0.940, loss: 0.217, lr: 0.0009903441445902451\n",
            "epoch: 197, acc: 0.940, loss: 0.216, lr: 0.0009902951079421667\n",
            "epoch: 198, acc: 0.941, loss: 0.216, lr: 0.0009902460761499234\n",
            "epoch: 199, acc: 0.940, loss: 0.216, lr: 0.0009901970492127933\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(200):\n",
        "    # Forward pass\n",
        "    dense1.forward(X_train)\n",
        "    activation1.forward(dense1.output)\n",
        "    dense2.forward(activation1.output)\n",
        "    loss = loss_activation.forward(dense2.output, y_train)\n",
        "\n",
        "    predictions = np.argmax(loss_activation.output, axis=1)\n",
        "    if len(y_train.shape) == 2:\n",
        "        y_train = np.argmax(y_train, axis=1)\n",
        "\n",
        "    accuracy = np.mean(predictions==y_train)\n",
        "\n",
        "    # if epoch % 100 == 0:\n",
        "    print(f'epoch: {epoch}, ' + f'acc: {accuracy:.3f}, ' + f'loss: {loss:.3f}, ' + f'lr: {optimizer.current_learning_rate}')\n",
        "\n",
        "    # Backward pass\n",
        "    loss_activation.backward(loss_activation.output, y_train)\n",
        "    dense2.backward(loss_activation.dinputs)\n",
        "    activation1.backward(dense2.dinputs)\n",
        "    dense1.backward(activation1.dinputs)\n",
        "\n",
        "    # Optimizing\n",
        "    optimizer.pre_update_params()\n",
        "    optimizer.update_params(dense1)\n",
        "    optimizer.update_params(dense2)\n",
        "    optimizer.post_update_params()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
